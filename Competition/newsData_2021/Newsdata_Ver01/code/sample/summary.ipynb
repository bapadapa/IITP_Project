{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#%%\r\n",
    "import numpy as np\r\n",
    "from numpy.core.fromnumeric import shape\r\n",
    "from numpy.core.numeric import NaN \r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from keras.layers import  LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\r\n",
    "from keras.models import Model, Input\r\n",
    "from keras.callbacks import EarlyStopping\r\n",
    "from keras import backend\r\n",
    "\r\n",
    "from scipy.sparse.construct import random\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from nltk.corpus import stopwords\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import urllib\r\n",
    "import warnings\r\n",
    "from tensorflow.python import keras\r\n",
    "\r\n",
    "from tensorflow.python.keras import activations\r\n",
    "# Warning 제거 및 칼럼 여러개 보기\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "pd.set_option(\"display.max_colwidth\", 200)\r\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\r\n",
    "from attention import AttentionLayer\r\n",
    "\r\n",
    "dt = pd.read_csv(r'..\\..\\data\\sample_data\\Reviews.csv')\r\n",
    "\r\n",
    "# 중복된 Text값 제거 \r\n",
    "dt.drop_duplicates(subset = ['Text'], inplace= True )\r\n",
    "# na값 제거\r\n",
    "dt.dropna(axis=0, inplace=True)\r\n",
    "\r\n",
    "# 영어 기준 축양형(줄인말) 사전 \r\n",
    "contraction_mapping = {\"isn't\":\"is not\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\"you're\": \"you are\", \"you've\": \"you have\"}\r\n",
    "\r\n",
    "# Text 전처리\r\n",
    "stop_words = set(stopwords.words('english'))\r\n",
    "def text_cleansing(text):\r\n",
    "    # 소문자 통일\r\n",
    "    newS = text.lower()\r\n",
    "    # HTML태그 삭제\r\n",
    "    newS = BeautifulSoup(newS, \"lxml\").text\r\n",
    "    # ()안의 단어 제거\r\n",
    "    newS = re.sub(r'\\([^)]*\\)','',newS)\r\n",
    "    # 따옴표 제거\r\n",
    "    newS = re.sub('\"','',newS)\r\n",
    "    # 축약어 제거\r\n",
    "    newS = ' ' .join(\r\n",
    "        [contraction_mapping[t] \r\n",
    "            if t in contraction_mapping \r\n",
    "            else t for t in newS.split(\" \")]\r\n",
    "        )\r\n",
    "    # 불필요 공백 제거\r\n",
    "    newS = re.sub(r\"s\\b\",\"\",newS)\r\n",
    "    # 특수문자 제거\r\n",
    "    newS = re.sub(\"[^a-zA-Z]\",\" \", newS)\r\n",
    "    # 불용어 제거\r\n",
    "    tokens = [w for w in newS.split() if not w in stop_words]\r\n",
    "    # 짧은 단어 제거\r\n",
    "    long_words = []\r\n",
    "    for i in tokens:\r\n",
    "        if len(i)>= 4:\r\n",
    "            long_words.append(i)\r\n",
    "    return (\" \".join(long_words)).strip()\r\n",
    "#정제된 Text 기존 데이터에 추가\r\n",
    "cleaned_text = []\r\n",
    "for t in dt['Text']:\r\n",
    "    cleaned_text.append(text_cleansing(t))\r\n",
    "dt['cleaned_text'] = cleaned_text \r\n",
    "\r\n",
    "# 요약 Text 정제\r\n",
    "def summary_cleansing(text):\r\n",
    "    newS = re.sub('\"','',text)\r\n",
    "    newS = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newS.split(\" \")])\r\n",
    "    newS = re.sub(r\"'s\\b\",\"\", newS)\r\n",
    "    newS = re.sub(\"[^a-zA-Z]\",\" \", newS)\r\n",
    "    newS = newS.lower()\r\n",
    "    tokens = newS.split()\r\n",
    "    newS = ''\r\n",
    "    for i in tokens:\r\n",
    "        if len(i) > 1:\r\n",
    "            newS += i+' '\r\n",
    "    return newS\r\n",
    "# 정제된 Text 추가\r\n",
    "cleaned_summary = []\r\n",
    "for t in dt['Summary']:\r\n",
    "    cleaned_summary.append(summary_cleansing(t))\r\n",
    "dt['cleaned_summary'] = cleaned_summary \r\n",
    "dt['cleaned_summary'].replace('',np.nan,inplace=True)\r\n",
    "dt.dropna(axis = 0 , inplace= True)\r\n",
    "dt['cleaned_summary']= dt['cleaned_summary'].apply(lambda x : '_start_ '+x+' _end_')\r\n",
    "\r\n",
    "\r\n",
    "# 정제된 텍스트 시각화 화여 적절한 반환 길이 추정\r\n",
    "# 필요에 따라 진행할것\r\n",
    "text_wc = []\r\n",
    "summary_wc = []\r\n",
    "\r\n",
    "for i in dt['cleaned_text']:\r\n",
    "    text_wc.append(len(i.split()))\r\n",
    "for i in dt['cleaned_summary']:\r\n",
    "    summary_wc.append(len(i.split()))\r\n",
    "length_df = pd.DataFrame({'Text' : text_wc,'summary': summary_wc})\r\n",
    "length_df.hist(bins= 20)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Plot을 본것을 토대로 Text 길이 지정\r\n",
    "max_text_len = 80\r\n",
    "max_summary_len = 20\r\n",
    "\r\n",
    "# Train, Test 만들기\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(\r\n",
    "    dt['cleaned_text'],\r\n",
    "    dt['cleaned_summary'],\r\n",
    "    train_size= 0.8,\r\n",
    "    random_state= 0,\r\n",
    "    shuffle= True\r\n",
    "    )\r\n",
    "\r\n",
    "def word_tokenizer(train , test,max_len):\r\n",
    "    tokenizer_ = Tokenizer()\r\n",
    "    tokenizer_.fit_on_texts(list(train))\r\n",
    "\r\n",
    "    train = tokenizer_.texts_to_sequences(train)\r\n",
    "    test = tokenizer_.texts_to_sequences(test)\r\n",
    "\r\n",
    "\r\n",
    "    train = pad_sequences(train, maxlen=max_len,padding='post')\r\n",
    "    test = pad_sequences(test, maxlen=max_len,padding='post')\r\n",
    "\r\n",
    "    vodc_size = len(tokenizer_.word_index)+1\r\n",
    "    return train,test,vodc_size,tokenizer_\r\n",
    "\r\n",
    "x_train, x_test,x_voc_size,x_tokenizer = word_tokenizer(x_train,x_test,max_text_len)\r\n",
    "y_train, y_test,y_voc_size,y_tokenizer = word_tokenizer(y_train,y_test,max_summary_len)\r\n",
    "\r\n",
    "# 모델생성\r\n",
    "# Keras 로 생성된것들 모두 초기화\r\n",
    "backend.clear_session()\r\n",
    "latent_dim = 512\r\n",
    "\r\n",
    "# Encoder \r\n",
    "encoder_inputs = Input(shape=(max_text_len,)) \r\n",
    "enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \r\n",
    "\r\n",
    "#LSTM 1 \r\n",
    "enc_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \r\n",
    "enc_output1, state_h1, state_c1 = enc_lstm1(enc_emb) \r\n",
    "\r\n",
    "#LSTM 2 \r\n",
    "enc_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \r\n",
    "enc_output2, state_h2, state_c2 = enc_lstm2(enc_output1) \r\n",
    "\r\n",
    "#LSTM 3 \r\n",
    "enc_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \r\n",
    "enc_outputs, state_h, state_c= enc_lstm3(enc_output2) \r\n",
    "\r\n",
    "# Set up the decoder. \r\n",
    "dec_inputs = Input(shape=(None,)) \r\n",
    "dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \r\n",
    "dec_emb = dec_emb_layer(dec_inputs) \r\n",
    "\r\n",
    "#LSTM using encoder_states as initial state\r\n",
    "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \r\n",
    "dec_outputs,decoder_fwd_state, decoder_back_state = dec_lstm(dec_emb,initial_state=[state_h, state_c]) \r\n",
    "\r\n",
    "#Attention Layer\r\n",
    "attn_layer = AttentionLayer(name='attention_layer') \r\n",
    "attn_out, attn_states = attn_layer([enc_outputs, dec_outputs]) \r\n",
    "4\r\n",
    "# Concat attention output and decoder LSTM output \r\n",
    "dec_concat_input = Concatenate(axis=-1, name='concat_layer')([dec_outputs, attn_out])\r\n",
    "\r\n",
    "#Dense layer\r\n",
    "dec_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \r\n",
    "dec_outputs = dec_dense(dec_concat_input) \r\n",
    "\r\n",
    "# Define the model\r\n",
    "model = Model([encoder_inputs, dec_inputs], dec_outputs) \r\n",
    "model.summary()\r\n",
    "\r\n",
    "model.compile(\r\n",
    "    optimizer = 'rmsprop',\r\n",
    "    loss = 'sparse_categorical_crossentropy'\r\n",
    ")\r\n",
    "#callnack 함수\r\n",
    "# EarlyStopping : 개선 없을시 fitting 중단\r\n",
    "es = EarlyStopping(\r\n",
    "    monitor= 'val_loss',\r\n",
    "    mode = 'min',\r\n",
    "    verbose = 1\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# with tf.device(\"/device:gpu:0\") :\r\n",
    "#     history=model.fit(\r\n",
    "#         [\r\n",
    "#             x_train,y_train[:,:-1]],\r\n",
    "#             y_train.reshape(y_train.shape[0],\r\n",
    "#             y_train.shape[1], 1)[:,1:] ,\r\n",
    "#         epochs=2,\r\n",
    "#         callbacks=[es],\r\n",
    "#         batch_size=128,\r\n",
    "#         validation_data=([x_test,y_test[:,:-1]], \r\n",
    "#         y_test.reshape(y_test.shape[0],\r\n",
    "#         y_test.shape[1], 1)[:,1:])\r\n",
    "#         )\r\n",
    "with tf.device(\"/device:gpu:0\") :\r\n",
    "    history=model.fit(\r\n",
    "        [\r\n",
    "            x_train,y_train[:,:-1]],\r\n",
    "            y_train.reshape(y_train.shape[0],\r\n",
    "            y_train.shape[1], 1)[:,1:] ,\r\n",
    "        epochs=2,\r\n",
    "        callbacks=[es],\r\n",
    "        batch_size=128,\r\n",
    "        validation_data=([x_test,y_test[:,:-1]], \r\n",
    "        y_test.reshape(y_test.shape[0],\r\n",
    "        y_test.shape[1], 1)[:,1:])\r\n",
    "        )\r\n",
    "\r\n",
    "\r\n",
    "# inx2word\r\n",
    "reverse_target_word_index = y_tokenizer.index_word\r\n",
    "reverse_source_word_index = y_tokenizer.index_word\r\n",
    "target_word_index = y_tokenizer.word_index\r\n",
    "\r\n",
    "enc_model = Model(inputs = encoder_inputs,outputs= [enc_outputs,state_h,state_c])\r\n",
    "\r\n",
    "dec_state_input_h = Input(shape=(latent_dim,))\r\n",
    "dec_state_input_c = Input(shape=(latent_dim,))\r\n",
    "dec_hid_state_input = Input(shape = (max_text_len, latent_dim))\r\n",
    "\r\n",
    "dec_emb2 = dec_emb_layer(dec_inputs)\r\n",
    "\r\n",
    "dec_output02,state_h2,state_c2 = dec_lstm(dec_emb2, initial_state = [dec_state_input_h,dec_state_input_c])\r\n",
    "\r\n",
    "attn_out_inf, attn_states_inf = attn_layer([dec_hid_state_input,dec_output02])\r\n",
    "dec_inf_con = Concatenate(axis=-1 , name = 'dec_inf_con')([dec_output02,attn_out_inf])\r\n",
    "\r\n",
    "dec_output02 = dec_dense(dec_inf_con)\r\n",
    "\r\n",
    "dec_model = Model(\r\n",
    "[dec_inputs] + [dec_hid_state_input,dec_state_input_h, dec_state_input_c],\r\n",
    "[dec_output02] + [state_h2, state_c2])\r\n",
    "\r\n",
    "def decode_sequence(input_seq):\r\n",
    "    # Encode the input as state vectors.\r\n",
    "    e_out, e_h, e_c = enc_model.predict(input_seq)\r\n",
    "\r\n",
    "    # Generate empty target sequence of length 1.\r\n",
    "    target_seq = np.zeros((1,1))\r\n",
    "\r\n",
    "    # Chose the 'start' word as the first word of the target sequence\r\n",
    "    target_seq[0, 0] = target_word_index['start']\r\n",
    "\r\n",
    "    stop_condition = False\r\n",
    "    decoded_sentence = ''\r\n",
    "    while not stop_condition:\r\n",
    "        output_tokens, h, c = dec_model.predict([target_seq] + [e_out, e_h, e_c])\r\n",
    "\r\n",
    "        # Sample a token\r\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\r\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\r\n",
    "\r\n",
    "        if(sampled_token!='end'):\r\n",
    "            decoded_sentence += ' '+sampled_token\r\n",
    "\r\n",
    "            # Exit condition: either hit max length or find stop word.\r\n",
    "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_summary_len-1)):\r\n",
    "                stop_condition = True\r\n",
    "\r\n",
    "        # Update the target sequence (of length 1).\r\n",
    "        target_seq = np.zeros((1,1))\r\n",
    "        target_seq[0, 0] = sampled_token_index\r\n",
    "\r\n",
    "        # Update internal states\r\n",
    "        e_h, e_c = h, c\r\n",
    "\r\n",
    "    return decoded_sentence\r\n",
    "\r\n",
    "def seq2summary(input_seq):\r\n",
    "    newString=''\r\n",
    "    for i in input_seq:\r\n",
    "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\r\n",
    "        newString=newString+reverse_target_word_index[i]+' '\r\n",
    "    return newString\r\n",
    "\r\n",
    "def seq2text(input_seq):\r\n",
    "    newString=''\r\n",
    "    for i in input_seq:\r\n",
    "      if(i!=0):\r\n",
    "        newString=newString+reverse_source_word_index[i]+' '\r\n",
    "    return newString\r\n",
    "\r\n",
    "for i in range(len(x_test)):\r\n",
    "  print(\"Review:\",seq2text(x_test[i]))\r\n",
    "  print(\"Original summary:\",seq2summary(y_test[i]))\r\n",
    "  print(\"Predicted summary:\",decode_sequence(x_test[i].reshape(1,max_text_len)))\r\n",
    "  print(\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}